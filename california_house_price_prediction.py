# -*- coding: utf-8 -*-
"""California_house_price_prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18u1J5JbVK7l09YMpIdYiXBnmF_MPldsn
"""

# Importing dependencies

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import mean_squared_error as mse

# Importing dataset
housing = pd.read_csv('housing.csv')

"""### Cleaning and Exploratory Data Analysis"""

housing.head()

housing.shape

housing.info()

# Checking for null values

housing.isnull().sum()

# Checking for duplicate values

housing.duplicated().sum()

"""We can observe that:
*  The dataset contains 20,640 samples and 8 features;
*  All features are numerical features encoded as floating number except **ocean_proximity**.
*  There are missing values in the **total_bedrooms** feature.
*  There are no duplicate rows.

### Data Analysis and Visualisations
"""

# Analysing the values of the ocean_proximity feature

print(housing['ocean_proximity'].value_counts())

housing.describe()

# Creating a histogram to understand the distribution

housing.hist(bins = 50, figsize = (10, 10)) # Bins - Number of chucks we split the data into

"""Insights:

*   Most of the houses in the dataset are located in the western part of California.
*  The houses are spread across the state, with clusters in specific regions.
*  The dataset contains houses of varying ages, with a significant number of older homes (around 50 years) and newer homes (around 20 years).
*   Most houses in the dataset are relatively small, but there are a few very large properties with a high number of rooms.
*   Most households have a mid-range income, and most houses are relatively affordable, but there are a few high-income households and high-value properties.


"""

housing.plot(kind = 'scatter', x = 'latitude', y = 'longitude', alpha = 0.2)
# alpha adjusts the transparency of the dots such they are obervable

'''
Insights:
We can observe that the graph looks like the map of california.
The dense areas are where we have more observations (points are overlaid on eachother).
'''

# Plotting a scatter plot the location with the population and median house price
housing.plot(kind = 'scatter',
             x = 'latitude', y = 'longitude',
             alpha = 0.2,
             s = housing['population'] / 100, # Size of the points is proportional to the population of the district
             label = 'population',
             c = 'median_house_value', # Color of each point based on the price
             cmap = plt.get_cmap('jet'), # Color map used to plot the points
             colorbar = True) # Scale to interpret the colours

# Observation:
# As we go inland the median value of the houses decreases as compared to the houses near the ocean.

# Detecting relationship between different features using a scatter plot

# Correlation matrix - shows how closely realated two variables are.
numerical_features = housing.select_dtypes(include = np.number)
correlation_matrix = numerical_features.corr()

# Observing only the price with all the other features
correlation_matrix['median_house_value'].sort_values(ascending = False)

"""*  The median_income is positively correlated with the median price, meaning, higher the income, the more expensive the house.

*  The latitude is negatively correlated with the median_price, meaning, the houses up north are cheaper compared to the ones in southern california.
"""

# Dealing with the missing values in the bedrooms

# Dropping the rows with missing values
df = housing.dropna(subset = ['total_bedrooms'])
df.shape

# Handing the categorical variable using get_dummies

dummies = pd.get_dummies(df['ocean_proximity']).astype(int)
dummies

df_processed = pd.concat([df, dummies], axis = 'columns')
df_processed.head()

# Dropping the ocean_proximity and one of the variables in dummies (to avoid multicollinearity)

# Dropping island beacause we have only 5 samples

housing_data = df_processed.drop(['ocean_proximity', 'ISLAND'], axis = 1)
housing_data.head()

"""### Preparing the data to be fed into the model"""

housing_data.shape

# Splitting the data into Training, Validation and testing sets
train_set, val_set, test_set = housing_data[1:17000], housing_data[17000:19000], housing_data[19000:]

# Splitting data and labels
X_train, y_train = train_set.drop('median_house_value', axis = 1), train_set['median_house_value']
X_val, y_val = val_set.drop('median_house_value', axis = 1), val_set['median_house_value']
X_test, y_test = test_set.drop('median_house_value', axis = 1), test_set['median_house_value']

# Standardising the data

standardize = StandardScaler()
X_train_std = standardize.fit_transform(X_train)
X_val_std = standardize.transform(X_val)
X_test_std = standardize.transform(X_test)

# Generating a historgram to observe the difference

pd.DataFrame(X_train_std).hist()

"""### Ordinary Least Squares Regression Model"""

# Training the model

ols = LinearRegression()
ols.fit(X_train_std, y_train)

# Display the intercept, coeffiecients and R-squared values of the model

print("The intercept is:", ols.intercept_)
print("\nThe coefficients are:\n", ols.coef_)
print("\nThe R-squared value for: ", ols.score(X_train_std, y_train))

# Performing prediction
y_pred = ols.predict(X_test_std)

# making a dataframe with the true value and the predicted value
performance = pd.DataFrame({'True_val': y_test, 'Pred_val': y_pred})
performance.head()

# Calculating the difference between the True value and prediction
performance['error'] = performance['True_val'] - performance['Pred_val']
performance.head()

# Plotting a graph to visualise the error

# Resetting the index and adding it as a column
performance.reset_index(drop = True, inplace = True)
performance.reset_index(inplace = True)

performance.head()

performance.shape

# Plotting the bar chart
fig = plt.figure(figsize = (8, 6))

plt.bar(performance['index'], performance['error'], data = performance, color = 'red', width = 0.5)
plt.xlabel('Index')
plt.ylabel('Error')
plt.show()

# Observing just a few observations - zooming into on the first 50 observation

plt.bar('index', 'error', data = performance[:50], color = 'black', width = 0.5)
plt.xlabel('Index')
plt.ylabel('Error')
plt.show()

print("The Mean error for the training set:", np.sqrt(mse(ols.predict(X_train_std), y_train)))
print("The Mean error for the Validation set:", np.sqrt(mse(ols.predict(X_val_std), y_val)))

"""This graph shows how the model underestimates or overestimates the price.
*   If the residual value is positive, the model has underestimated the price.
*   If the residual value is negative, the model has overestimated the price of the house.

### Random Forest Model
"""

rfr = RandomForestRegressor(max_depth = 5).fit(X_train_std, y_train)

print("The mean error for the training data:", np.sqrt(mse(rfr.predict(X_train_std), y_train)))
print("The mean error for the validation data:", np.sqrt(mse(rfr.predict(X_val_std), y_val)))

"""The Random Forest model showed signs of overfitting, as the validation error was higher than the training error.

### Using Neural Network Models
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

# Simple Neural Net
simple_nn = Sequential()
simple_nn.add(InputLayer(input_shape = (X_train_std.shape[1],)))
simple_nn.add(Dense(32, 'relu'))
simple_nn.add(Dense(16, 'relu'))
simple_nn.add(Dense(1,'linear'))

opt = Adam(learning_rate = 0.5)
cp = ModelCheckpoint('models/simple_nn.keras', save_best_only = True)

simple_nn.compile(optimizer = opt, loss = 'mse', metrics = [RootMeanSquaredError()])
simple_nn.fit(x = X_train_std, y = y_train, validation_data = (X_val_std, y_val), callbacks = [cp], epochs = 10)

print("The mean error for the training data:", np.sqrt(mse(simple_nn.predict(X_train_std), y_train)))
print("The mean error for the validation data:", np.sqrt(mse(simple_nn.predict(X_val_std), y_val)))

"""We find that the Ordinary Linear Least Squares model has performed better in this case."""

print("The Mean error for the Test set:", np.sqrt(mse(ols.predict(X_test_std), y_test)))

"""This model can help real estate companies and policymakers understand the factors driving housing prices and make informed decisions about property investments and urban planning."""